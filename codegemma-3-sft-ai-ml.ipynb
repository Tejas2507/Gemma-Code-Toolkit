{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13047922,"sourceType":"datasetVersion","datasetId":8261067}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:31:58.442305Z","iopub.execute_input":"2025-09-14T21:31:58.442483Z","iopub.status.idle":"2025-09-14T21:32:00.704567Z","shell.execute_reply.started":"2025-09-14T21:31:58.442463Z","shell.execute_reply":"2025-09-14T21:32:00.703912Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/aiml-codebase-dataset/ai_ml_training_dataset.jsonl\n/kaggle/input/aiml-codebase-dataset/ai_ml_training_chat.csv\n/kaggle/input/aiml-codebase-dataset/ai_ml_raw_dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\ntry:\n    user_secrets = UserSecretsClient()\n    # Using your variable to hold the token\n    secret_value_1 = user_secrets.get_secret(\"HF_TOKEN\")\n    \n    # Passing your variable to the login function\n    login(token=secret_value_1)\n    \n    print(\"✅ Successfully logged in to Hugging Face Hub using the token.\")\nexcept Exception as e:\n    print(f\"⚠️ Could not log in. Please ensure 'HF_TOKEN' is a valid secret. Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:32:00.706547Z","iopub.execute_input":"2025-09-14T21:32:00.706840Z","iopub.status.idle":"2025-09-14T21:32:01.508272Z","shell.execute_reply.started":"2025-09-14T21:32:00.706822Z","shell.execute_reply":"2025-09-14T21:32:01.507438Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully logged in to Hugging Face Hub using the token.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q -U transformers datasets accelerate peft bitsandbytes trl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:32:01.509103Z","iopub.execute_input":"2025-09-14T21:32:01.509739Z","iopub.status.idle":"2025-09-14T21:33:45.912097Z","shell.execute_reply.started":"2025-09-14T21:32:01.509699Z","shell.execute_reply":"2025-09-14T21:33:45.911126Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer , SFTConfig\n\n# 1. Configuration\n# Model and tokenizer names\nmodel_name = \"google/codegemma-7b-it\"\n\n# Dataset path\n# 💡 IMPORTANT: Update this to the path where you uploaded your dataset.jsonl file\ndataset_path = \"/kaggle/input/aiml-codebase-dataset/ai_ml_training_dataset.jsonl\" \n\n# Fine-tuned model name\nnew_model_name = \"CodeGemma-7B-Docstring-Generator\"\n\n# 2. Define the Prompt Template Function (UPDATED)\ndef format_chat_template(sample):\n    \"\"\"\n    Formats the sample data from the JSONL file into the CodeGemma chat template.\n    This function is specifically designed to handle the {\"messages\": [...]} structure.\n    \"\"\"\n    # Extract user content and assistant content\n    user_message = sample['messages'][0]['content']\n    assistant_message = sample['messages'][1]['content']\n    \n    # Apply the CodeGemma chat template\n    formatted_prompt = f\"<start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n{assistant_message}<end_of_turn>\"\n    \n    return {\"text\": formatted_prompt}\ndataset = load_dataset('json', data_files=dataset_path, split='train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:33:45.913354Z","iopub.execute_input":"2025-09-14T21:33:45.913602Z","iopub.status.idle":"2025-09-14T21:34:17.730628Z","shell.execute_reply.started":"2025-09-14T21:33:45.913573Z","shell.execute_reply":"2025-09-14T21:34:17.729894Z"}},"outputs":[{"name":"stderr","text":"2025-09-14 21:33:58.131913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757885638.397031      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757885638.468507      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41576b97a82144a4a6a90dfcbac04ba2"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"dataset = dataset.select(range(1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:34:17.731503Z","iopub.execute_input":"2025-09-14T21:34:17.731823Z","iopub.status.idle":"2025-09-14T21:34:17.738656Z","shell.execute_reply.started":"2025-09-14T21:34:17.731805Z","shell.execute_reply":"2025-09-14T21:34:17.737810Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset = dataset.map(format_chat_template)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:34:17.739377Z","iopub.execute_input":"2025-09-14T21:34:17.739613Z","iopub.status.idle":"2025-09-14T21:34:17.883113Z","shell.execute_reply.started":"2025-09-14T21:34:17.739594Z","shell.execute_reply":"2025-09-14T21:34:17.882130Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b008c82ff8bf4b9bb04684a3124c4f98"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"\n# 3. Load Dataset and Apply Template\n\n# 4. Configure Quantization \n# Use 4-bit quantization to fit the model on T4 GPUs\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# 5. Load Base Model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    \n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# 6. Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# Set a padding token if one is not already defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# 7. PEFT/LoRA Configuration\n# LoRA enhances training efficiency by updating only a small subset of model parameters\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=16,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\noutput_dir = \"/kaggle/working/results\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 8. Training Arguments\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    warmup_steps=5,\n    num_train_epochs=1,\n    logging_steps=10,\n    save_steps=50,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    max_length=1024,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={'use_reentrant':False},\n    report_to=\"none\",\n    disable_tqdm=False,\n    packing = False,\n)\n\n# 9. Initialize the Trainer\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    formatting_func=format_chat_template,\n    args=training_arguments,\n)\n\n# 10. Start Training\nprint(\"🚀 Starting training...\")\ntrainer.train()\nprint(\"✅ Training complete!\")\n\n# 11. Save the Fine-Tuned Model Adapter\ntrainer.model.save_pretrained(new_model_name)\ntokenizer.save_pretrained(new_model_name)\nprint(f\"✅ Model saved to {new_model_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:34:17.885058Z","iopub.execute_input":"2025-09-14T21:34:17.885492Z","iopub.status.idle":"2025-09-15T03:03:05.649255Z","shell.execute_reply.started":"2025-09-14T21:34:17.885467Z","shell.execute_reply":"2025-09-15T03:03:05.648583Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e5f8093e8c548ce92a97e1495918c49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c5030f96cc42649434628158e6380d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907b921a2ee94ffda88ba7a55ec9497f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ece0cd20d224e3faeea885227ee60ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b618b2f5c94b8e9b55572bd57a370a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588d60d51abe488f9a70926d2207dc9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6571df0e73c8408f910e7cf4f4d2b594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39ecf939b6e444f8bcdac3e6657085d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"175d6cf38b854493aeb1c899242266de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a8ec0f82c2493782f7ad9c7257ae2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d027f56223754c638ac13fd8fa980a94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e310530709c4960abf63e47dc1450a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69936c926b864940bf8352059fd3083b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying formatting function to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128dec04cc86491bba735be111b7a8d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f87d3b768b46e98825f1463f0ece85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db23a1c78eff4914824251006e567fa8"}},"metadata":{}},{"name":"stdout","text":"🚀 Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 5:21:02, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.319200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.729200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.666900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.626200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.561000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.568500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ Training complete!\n✅ Model saved to CodeGemma-7B-Docstring-Generator\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"🚀 Pushing model to the Hub...\")\n\n# Define your Hugging Face repository ID\n# IMPORTANT: Replace \"YourHuggingFaceUsername\" with your actual username.\nrepo_id = \"Tejas1615/CodeGemma-7B-Docstring-Generator\" \n\n# Push the model adapter. It will create the repo if it doesn't exist.\n# Use private=True if you want to keep the model private.\ntrainer.model.push_to_hub(repo_id, private=False)\n\n# Push the tokenizer\ntrainer.tokenizer.push_to_hub(repo_id, private=False)\n\nprint(f\"✅ Model successfully pushed to https://huggingface.co/{repo_id}\")\n# ---------------------------------------------------------------- #","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T03:03:05.649951Z","iopub.execute_input":"2025-09-15T03:03:05.650137Z","iopub.status.idle":"2025-09-15T03:03:19.924431Z","shell.execute_reply.started":"2025-09-15T03:03:05.650122Z","shell.execute_reply":"2025-09-15T03:03:19.923547Z"}},"outputs":[{"name":"stdout","text":"🚀 Pushing model to the Hub...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/200M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8911c5da4ca74feca02c5cf64a49fc6f"}},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba692d396ce84fea8e0c777b560f81cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/38.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31fed27ab21e48a9a978dd11cf93b0b4"}},"metadata":{}},{"name":"stdout","text":"✅ Model successfully pushed to https://huggingface.co/Tejas1615/CodeGemma-7B-Docstring-Generator\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}