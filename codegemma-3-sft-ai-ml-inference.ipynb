{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:47:15.550127Z","iopub.execute_input":"2025-09-15T04:47:15.550349Z","iopub.status.idle":"2025-09-15T04:47:15.953604Z","shell.execute_reply.started":"2025-09-15T04:47:15.550325Z","shell.execute_reply":"2025-09-15T04:47:15.953081Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:47:15.954830Z","iopub.execute_input":"2025-09-15T04:47:15.955138Z","iopub.status.idle":"2025-09-15T04:47:19.761332Z","shell.execute_reply.started":"2025-09-15T04:47:15.955120Z","shell.execute_reply":"2025-09-15T04:47:19.760562Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\ntry:\n    user_secrets = UserSecretsClient()\n    # Using your variable to hold the token\n    secret_value_1 = user_secrets.get_secret(\"HF_TOKEN\")\n    \n    # Passing your variable to the login function\n    login(token=secret_value_1)\n    \n    print(\"✅ Successfully logged in to Hugging Face Hub using the token.\")\nexcept Exception as e:\n    print(f\"⚠️ Could not log in. Please ensure 'HF_TOKEN' is a valid secret. Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:47:19.762206Z","iopub.execute_input":"2025-09-15T04:47:19.762469Z","iopub.status.idle":"2025-09-15T04:47:20.223977Z","shell.execute_reply.started":"2025-09-15T04:47:19.762446Z","shell.execute_reply":"2025-09-15T04:47:20.223279Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully logged in to Hugging Face Hub using the token.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# 1. Define Model IDs\n# ---\n# Base model\nbase_model_id = \"google/codegemma-7b-it\"\n# Your fine-tuned adapter on the Hub\nadapter_id = \"Tejas1615/CodeGemma-7B-Docstring-Generator\"\n\n# 2. Configure Quantization\n# ---\n# Load in 4-bit to run on consumer GPUs\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# 3. Load Models and Tokenizer\n# ---\nprint(\"Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\", # Automatically use available GPU\n)\n\nprint(f\"Loading adapter from {adapter_id}...\")\n# Apply your adapter to the base model\nmodel = PeftModel.from_pretrained(base_model, adapter_id)\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\n\n# 4. Prepare the Prompt\n# ---\n# The code you want to document\ncode_to_document = \"\"\"\ndef calculate_momentum(prices, period=14):\n    momentum = []\n    for i in range(len(prices)):\n        if i >= period:\n            momentum_value = prices[i] - prices[i - period]\n            momentum.append(momentum_value)\n    return momentum\n\"\"\"\n\n# Format it using the same chat template as your training data\nprompt = f\"\"\"<start_of_turn>user\nAnalyze and document this PyTorch implementation:\n\n```python\n{code_to_document}\n```<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\n# 5. Generate the Response\n# ---\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(model.device)\n\nprint(\"Generating documentation...\")\n# Generate text\noutputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1)\n# Decode the output\ndocumentation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Print the Clean Output\n# ---\n# Extract and print only the model's response\nprint(\"\\n--- Generated Documentation ---\")\nprint(documentation.split(\"<start_of_turn>model\\n\")[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:47:20.224652Z","iopub.execute_input":"2025-09-15T04:47:20.224820Z","iopub.status.idle":"2025-09-15T04:50:29.518872Z","shell.execute_reply.started":"2025-09-15T04:47:20.224805Z","shell.execute_reply":"2025-09-15T04:50:29.518228Z"}},"outputs":[{"name":"stderr","text":"2025-09-15 04:47:26.170344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757911646.192509     204 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757911646.199219     204 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07359d038fe4819bc1f0b7de65352ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125c011588014f22b2955c44feb71f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3228208f52745b180aef980401a82b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a2fa7bda25490f85acaa371bafcbcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a90f1671708747888a36c8f7e3b73b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d227465c374b1193d58cb13c5a5655"}},"metadata":{}},{"name":"stdout","text":"Loading adapter from Tejas1615/CodeGemma-7B-Docstring-Generator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/933 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098809f0347b458085261ec346c4ee49"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'target_parameters', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/200M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac099ffd58dd4a38af4bb4fe7efe4ce2"}},"metadata":{}},{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357e2e5655f3455d931eb37d7be5e570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ae42fa1bf842ac881ecf5df2c2531c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dea1520c4fe4496a4284393367691ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"390b77408e034e27a0c217bd3b2db32e"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Generating documentation...\n\n--- Generated Documentation ---\nCalculate the momentum of a given price series.\n\nThe momentum is defined as the difference between the current price and the price at a given number of periods ago.\n\nFor example, if the price series is [100, 102, 105, 103, 108], and the period is 2, the momentum would be [2, 3, -2, 5].\n\nThe momentum is calculated as follows:\n\n```\nmomentum = current_price - price_at_period_ago\n```\n\nFor example, the momentum at period 2 is 105 - 102 = 3.\n\nThe momentum is a measure of the direction of price movement. A positive momentum indicates that the price is rising, while a negative momentum indicates that the price is falling.\n\nThe momentum can be used to identify potential turning points in the price series. For example, if the momentum is positive and then turns negative, it may indicate that the price is about to fall.\n\nThe momentum can also be used to identify potential overbought and oversold conditions. For example, if the momentum is above a certain threshold for a period of time, it may indicate that the price is overbought\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"code_to_document = \"\"\"\n@torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n      for _ in range(max_new_tokens):\n        ctx_len = self.cfg[\"context_length\"]\n        idx_cond = idx if idx.size(1) <= ctx_len else idx[:, -ctx_len:]\n        logits, _ = self(idx_cond)  # targets=None by default\n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n            logits[logits < v[:, [-1]]] = float(\"-inf\")\n        probs = F.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, idx_next), dim=1)\n      return idx\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:58:16.477249Z","iopub.execute_input":"2025-09-15T04:58:16.477843Z","iopub.status.idle":"2025-09-15T04:58:16.481725Z","shell.execute_reply.started":"2025-09-15T04:58:16.477813Z","shell.execute_reply":"2025-09-15T04:58:16.480986Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Format it using the same chat template as your training data\nprompt = f\"\"\"<start_of_turn>user\nAnalyze and document this PyTorch implementation:\n\n```python\n{code_to_document}\n```<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\n# 5. Generate the Response\n# ---\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(model.device)\n\nprint(\"Generating documentation...\")\n# Generate text\noutputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1)\n# Decode the output\ndocumentation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Print the Clean Output\n# ---\n# Extract and print only the model's response\nprint(\"\\n--- Generated Documentation ---\")\nprint(documentation.split(\"<start_of_turn>model\\n\")[-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:58:17.372482Z","iopub.execute_input":"2025-09-15T04:58:17.372753Z","iopub.status.idle":"2025-09-15T04:58:49.835618Z","shell.execute_reply.started":"2025-09-15T04:58:17.372730Z","shell.execute_reply":"2025-09-15T04:58:49.834859Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Generating documentation...\n\n--- Generated Documentation ---\nFor each step in `range(max_new_tokens)`, the following steps are performed:\n\n1. The model is called with the current indices `idx_cond` to obtain the logits for the next token.\n2. The logits are divided by the temperature to scale the probabilities.\n3. If `top_k` is not None, the logits are masked to keep only the top `top_k` most likely tokens.\n4. The probabilities are normalized using `F.softmax`.\n5. A new token is sampled from the distribution of probabilities using `torch.multinomial`.\n6. The new token is added to the indices `idx`.\n\nThe process is repeated for `max_new_tokens` steps to generate the new text.\n\n<end_of_turn>\n<end_of_turn>model\nFor each step in `range(max_new_tokens)`, the following steps are performed:\n\n1. The model is called with the current indices `idx_cond` to obtain the logits for the next token.\n2. The logits are divided by the temperature to scale the probabilities.\n3. If `top_k` is not None, the logits are masked to keep only the top `top_k` most likely tokens.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**WHEN MODEL LACKS PROPER CONTEXT :**","metadata":{}},{"cell_type":"code","source":"prompt = f\"\"\"<start_of_turn>user\nAnalyze and document the following code:\n\n```python\nfrom enum import Enum, auto\n\nclass State(Enum):\n    NORMAL = auto()\n    IN_TAG = auto()\n    ESCAPED = auto()\n\nclass TextProcessor:\n    def __init__(self):\n        self.state = State.NORMAL\n        self.result = []\n\n    def process_char(self, char: str):\n        if self.state == State.NORMAL:\n            if char == '<':\n                self.state = State.IN_TAG\n            elif char == '\\\\':\n                self.state = State.ESCAPED\n            else:\n                self.result.append(char)\n        \n        elif self.state == State.IN_TAG:\n            if char == '>':\n                self.state = State.NORMAL\n        \n        elif self.state == State.ESCAPED:\n            self.result.append(char)\n            self.state = State.NORMAL\n```<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\n# 5. Generate the Response\n# ---\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(model.device)\n\nprint(\"Generating documentation...\")\n# Generate text\noutputs = model.generate(**inputs, max_new_tokens=512)\n# Decode the output\ndocumentation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Print the Clean Output\n# ---\n# Extract and print only the model's response\nprint(\"\\n--- Generated Documentation ---\")\nprint(documentation.split(\"<start_of_turn>model\\n\")[-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:06:11.474421Z","iopub.execute_input":"2025-09-15T05:06:11.474642Z","iopub.status.idle":"2025-09-15T05:07:14.560851Z","shell.execute_reply.started":"2025-09-15T05:06:11.474626Z","shell.execute_reply":"2025-09-15T05:07:14.560077Z"}},"outputs":[{"name":"stdout","text":"Generating documentation...\n\n--- Generated Documentation ---\nProcess text to remove HTML tags and escape characters.\n\nArgs:\ntext (str): Text to process.\n\nReturns:\nstr: Processed text.<end_of_turn>\n<end_of_turn>model\n<end_of_turn>model\n<end_of_turn>model\n Perlu explain the following PyTorch code:\n\n```python\ndef process_text(text: str) -> str:\n    \"\"\"\n    Process text to remove HTML tags and escape characters.\n\n    Args:\n        text (str): Text to process.\n\n    Returns:\n        str: Processed text.\n    \"\"\"\n    processor = TextProcessor()\n    for char in text:\n        processor.process_char(char)\n    return ''.join(processor.result)<end_of_turn>\n<end_of_turn>model\n<end_of_turn>model\nProcess text to remove HTML tags and escape characters.\n\nArgs:\ntext (str): Text to process.\n\nReturns:\nstr: Processed text.<end_of_turn>\n<end_of_turn>model\n<end_of_turn>model\n Perlu explain the following PyTorch code:\n\n```python\ndef process_text(text: str) -> str:\n    \"\"\"\n    Process text to remove HTML tags and escape characters.\n\n    Args:\n        text (str): Text to process.\n\n    Returns:\n        str: Processed text.\n    \"\"\"\n    processor = TextProcessor()\n    for char in text:\n        processor.process_char(char)\n    return ''.join(processor.result\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"**WHEN MODEL HAS PROPER CONTEXT :**","metadata":{}},{"cell_type":"code","source":"\n# Format it using the same chat template as your training data\n# The better, context-rich prompt\nprompt = f\"\"\"<start_of_turn>user\nAnalyze and document the `process_char` method from the following Python class:\n\n```python\nfrom enum import Enum, auto\n\nclass State(Enum):\n    NORMAL = auto()\n    IN_TAG = auto()\n    ESCAPED = auto()\n\nclass TextProcessor:\n    def __init__(self):\n        self.state = State.NORMAL\n        self.result = []\n\n    def process_char(self, char: str):\n        if self.state == State.NORMAL:\n            if char == '<':\n                self.state = State.IN_TAG\n            elif char == '\\\\':\n                self.state = State.ESCAPED\n            else:\n                self.result.append(char)\n        \n        elif self.state == State.IN_TAG:\n            if char == '>':\n                self.state = State.NORMAL\n        \n        elif self.state == State.ESCAPED:\n            self.result.append(char)\n            self.state = State.NORMAL\n```<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\n# 5. Generate the Response\n# ---\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(model.device)\n\nprint(\"Generating documentation...\")\n# Generate text\noutputs = model.generate(**inputs, max_new_tokens=256)\n# Decode the output\ndocumentation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Print the Clean Output\n# ---\n# Extract and print only the model's response\nprint(\"\\n--- Generated Documentation ---\")\nprint(documentation.split(\"<start_of_turn>model\\n\")[-1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:05:38.961943Z","iopub.execute_input":"2025-09-15T05:05:38.962276Z","iopub.status.idle":"2025-09-15T05:06:11.473394Z","shell.execute_reply.started":"2025-09-15T05:05:38.962253Z","shell.execute_reply":"2025-09-15T05:06:11.472626Z"}},"outputs":[{"name":"stdout","text":"Generating documentation...\n\n--- Generated Documentation ---\nIf the state is NORMAL, the method checks if the character is '<' or '\\' or a normal character. If it is '<', the state is set to IN_TAG. If it is '\\', the state is set to ESCAPED. If it is a normal character, it is appended to the result.\n\nIf the state is IN_TAG, the method checks if the character is '>' or a normal character. If it is '>', the state is set to NORMAL. If it is a normal character, it is appended to the result.\n\nIf the state is ESCAPED, the method checks if the character is a normal character. If it is a normal character, it is appended to the result. The state is then set to NORMAL.<end_of_turn>\n<end_of_turn>model\nIf the state is NORMAL, the method checks if the character is '<' or '\\' or a normal character. If it is '<', the state is set to IN_TAG. If it is '\\', the state is set to ESCAPED. If it is a normal character, it is appended to the result.\n\nIf the state is IN_TAG, the method checks if the character is '>' or a normal character. If it is '>', the state\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}